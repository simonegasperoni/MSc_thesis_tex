\documentclass{article}
\usepackage{amsthm}
\usepackage[utf8x]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\author{Simone Gasperoni}
\theoremstyle{plain}
\newtheorem{thm}{Teorema}[] 
\theoremstyle{definition}
\newtheorem{defn}[]{Definizione} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\author{Simone Gasperoni}
\title{title}

\begin{document}

\begin{titlepage}
\begin{center}

\textsc{Laurea magistrale in Ingegneria informatica (D.M. 270)}\\[0.5cm]
\textsc{Università degli studi di Roma Tre}\\[0.5cm]

\hrulefill

{ \huge \bfseries title \\[0.4cm] }
\textsc{\Large Simone Gasperoni}\\[0.5cm]
\vfill
\LaTeX


Roma, Italia
\end{center}
\end{titlepage}


\tableofcontents

\newpage
\section{Introduzione}
\textbf{\textit{I dati sono aperti se chiunque è libero di accederli, usarli, modificarli, e condividerli.}}
\footnotemark
\footnotetext{http://opendefinition.org/od/2.1/en}

\subsection{La filosofia open-data}

Gli aspetti più importanti della filosofia open-data sono:
\begin{itemize}
\item Disponibilità e accesso.
\item Riutilizzo e ridistribuzione: i dati devono essere forniti a condizioni tali da permetterne il riutilizzo e la ridistribuzione. Ciò comprende la possibilità di combinarli con altre basi di dati.
\item Partecipazione universale: tutti devono essere in grado di usare, riutilizzare e ridistribuire i dati.
\end{itemize}

Il movimento open-data si diffuse per la prima volta a seguito della promulgazione della Direttiva sull'Open government del 2009 negli Stati Uniti d'America:
\textit{Fin dove possibile e sottostando alle sole restrizioni valide, le agenzie devono pubblicare le informazioni on line utilizzando un formato aperto (open) che possa cioè essere recuperato, soggetto ad azioni di download, indicizzato e ricercato attraverso le applicazioni di ricerca web più comunemente utilizzate. Per formato open si intende un formato indipendente rispetto alla piattaforma, leggibile dall'elaboratore e reso disponibile al pubblico senza che sia impedito il riuso dell'informazione veicolata.}
\\
In questo contesto è nato il portale Data.gov, creato con l'obiettivo di raccogliere in un unico portale tutte le informazioni rese disponibili dagli enti statunitensi in formato aperto. In Italia il portale che raccoglie i dati aperti della pubblica amministrazione è Dati.gov.it
 
\subsection{Interoperabilità}
Tra le ragioni più importanti della filosofia open data abbiamo l'interoperabilità, vale a dire, la capacità di diversi sistemi e organizzazioni di lavorare insieme. Nel caso specifico degli open data - ovviamente - ci riferiamo alla capacità di combinare una grande quantità di dati da diverse fonti. L'interoperabilità è importante perché permette a componenti diverse di lavorare insieme. L'abilità di rendere ciascun dato un componente e di combinare insieme vari componenti è essenziale per la costruzione di sistemi sofisticati. In assenza di interoperabilità ciò diventa quasi impossibile - come nel mito della Torre di Babele, in cui l'impossibilità di comunicare (e quindi di Inter-operare) dà luogo a un fallimento sistemico della costruzione della torre. Una chiara definizione di \say{apertura} assicura che sia possibile combinare dataset aperti provenienti da fonti diverse in modo adeguato, evitando così la \say{Torre di Babele}.
\\
Il punto cruciale di un bacino di dati (o linee di codice) accessibili e utilizzabili in modo condiviso è il fatto che potenzialmente possono essere liberamente \say{mescolati} con dati provenienti da fonti anch'esse aperte. L'interoperabilità è la chiave per realizzare il principale vantaggio pratico dell'apertura: aumenta in modo esponenziale la possibilità di combinare diverse basi di dati, e quindi sviluppare nuovi e migliori prodotti e servizi.
\footnotemark
\footnotetext{http://opendatahandbook.org/guide/it/what-is-open-data}
\\
\\
DCAT è un vocabolario RDF progettato per facilitare l'interoperabilità tra cataloghi di dati pubblicati sul web. DCAT incentiva la pubblicazione decentrata di cataloghi e facilita la ricerca federata di dati tra i siti. Usando lo standard DCAT chi pubblica insiemi di dati incrementa la propria visibilità e favorisce le applicazioni che lavorano su dati e metadati in cataloghi multipli (applicazioni cross-portali).

\subsection{CKAN}
CKAN acronimo di Comprehensive Knowledge Archive Network è una sistema open source (https://github.com/ckan/ckan) pensato per l'immagazzinamento la, la catalogazione e la distribuzione dei dati in diversi formati; quali ad esempio fogli di calcolo, csv, pdf, xml, rfd, ... CKAN è scritto in Python ed è  è ispirato dal sistema di gestione dei pacchetti comune a sistemi operativi open source come quelli della famiglia Linux.
\\
Il codice base di CKAN viene mantenuto dalla Open Knowledge Foundation. Il sistema è usato sia come piattaforma pubblica su Datahub (https://datahub.io), sia da varie pubbliche amministrazioni nell'ambito della loro strategia di pubblicazione di dati aperti, ogni portale web basato su CKAN espone delle Application programming interface API mediante le quali è possibile interrogare i dataset e fruire dei dati di ciascun portale. 
\\
\\
Utilizzare 

\subsection{DCAT, DCAT-AP e DCAT-AP-it}

Il DCAT Application Profile (DCAT-AP) fornisce una specifica comune per la descrizione di dati della pubblica amministrazione in Europa, questa specifica è basata sullo standard DCAT: Data Catalog Vocabulary. \footnotemark
\footnotetext{https://www.w3.org/TR/vocab-dcat}
\\
DCAT-AP-it è la versione italiana di DCAT-AP, il profilo nazionale dei metadati utili per descrivere i dati delle pubbliche amministrazioni, conforme alla specifica di DCAT-AP serve a favorire l'interoperabilità semantica di dati e servizi. DCAT-AP-it è un modello dati pensato per rendere omogenei in tutta la pubblica amministrazione italiana i processi di accesso e scambio delle informazioni tra le istituzioni stesse e tra le istituzioni e i cittadini e le imprese, in coerenza con il relativo framework europeo. L'utilizzo di un modello dati valorizza il patrimonio informativo pubblico nazionale in linea con la Direttiva relativa al riutilizzo dell'informazione del settore pubblico.
Le categorie - definite dal portale europeo European Data Portal - sono le seguenti:
\footnotemark
\footnotetext{DCAT-AP-it, Profilo italiano di DCAT-AP, Versione 1.0, Agenzia per l'italia digitale}
\begin{enumerate}
\item Agriculture, Fisheries, Forestry \& Foods
\item Energy
\item Regions \& Cities
\item Transport
\item Economy \& Finance
\item International Issues
\item Government \& Public Sector
\item Justice, Legal System \& Public Safety
\item Education, Culture \& Sport
\item Environment
\item Health
\item Population \& Society
\item Science \& Technology
\end{enumerate}


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.40]{img/DCAT-AP_IT_UML.png}
\caption{Modello dati DCAT-AP-it}
\end{center}
\end{figure}

\newpage
\subsection{EuroVoc e JRC-Acquis}
EuroVoc è un thesaurus multilingue e pluridisciplinare che comprende la terminologia dei settori d'attività dell'Unione europea. Contiene termini in 23 lingue dell'UE. EuroVoc è in linea con le raccomandazioni del W3C e con gli ultimi sviluppi negli standard di classificazione. Il thesaurus EuroVoc viene utilizzato dalle istituzioni dell'Unione europea, dall'Ufficio delle pubblicazioni dell'UE, da parlamenti nazionali e regionali in Europa, come pure da amministrazioni nazionali e utenti privati di tutto il mondo.
\footnotemark
\footnotetext{http://eurovoc.europa.eu}
I descrittori del thesaurus multilingua EuroVoc sono usati da molti parlamentari europei e centri di documentazione per indicizzare manualmente le loro collezioni di documenti, i descrittori assegnati ai documenti sono utilizzati per cercare ed individuare documenti in una gerarchia semantica divisa in 21 domini, 127 sottodomini (microtesauri) e circa 7000 descrittori (concetti).
\\
\\
La versione corrente di JRC-Acquis è la 3.0, i vantaggi che possiamo trarre utilizzando JRC-Acquis come corpus di addestramento per la catalogazione di open data sono molteplici. Acquis rilascia periodicamente nuove versioni di corpus indicizzati con i descrittori di EuroVoc.
\\
\\
Tra i vantaggi principali abbiamo: 

\begin{enumerate}
\item Standard TEI
\item Corpus aggiornato e manutenuto
\item Corpus multilingua
\item Bridge dinamico tra i concetti e le categorie DCAT-AP
\end{enumerate}

\subsubsection{Standard TEI}
JRC-Acquis ha un formato che rispetta lo standard TEI.
Text Encoding Initiative (TEI) è un consorzio che sviluppa e mantiene uno standard per la rappresentazione di testi in formato digitale. La codifica TEI garantisce che il corpus sia machine-readable,  le modalità di interazione dei sistemi che utilizzano JRC-Acquis (ad esempio un modulo XQuery) non variano al variare delle versioni di JRC-Acquis.
\\
\\
Il titolo del documento è contenuto nel tag head:
\begin{verbatim}
<head n="1"> 
   Decisione n. 2046/2002/CE del Parlamento europeo e del Consiglio,
   del 21 ottobre 2002, che modifica la decisione n. 1719/1999/CE
   relativa ad una serie di orientamenti, compresa l'individuazione 
   di progetti di interesse comune, per reti transeuropee per lo 
   scambio elettronico di dati fra amministrazioni (IDA)
</head>
\end{verbatim}
Il corpo testuale del documento è contenuto nel tag div di tipo body
\begin{verbatim}
<div type="body">
   ...
</div>
\end{verbatim}
Segue la parte relativa alla classificazione EUROVOC:
\begin{verbatim}
<profileDesc>
   <textClass>
      <classCode scheme="eurovoc">206</classCode>
      <classCode scheme="eurovoc">3010</classCode>
      <classCode scheme="eurovoc">453</classCode>
      <classCode scheme="eurovoc">616</classCode>
      <classCode scheme="eurovoc">4424</classCode>
      <classCode scheme="eurovoc">5864</classCode>
   </textClass>
</profileDesc>
\end{verbatim}
Ogni documento contiene anche metadati che potrebbero essere utili come la data di emissione dei documenti:
\begin{verbatim}
<date>
   2007-03-29
</date>
\end{verbatim}
e la sorgente:
\begin{verbatim}
<div type="signature">
   <p n="77">Fatto a Lussemburgo, addì 21 ottobre 2002.</p>
   <p n="78">Per il Parlamento europeo</p>
   <p n="79">Il Presidente</p>
   <p n="80">P. Cox</p>
   <p n="81">Per il Consiglio</p>
   <p n="82">Il Presidente</p>
   <p n="83">P. S. Møller</p>
   <p n="84">(1) GU C 332 E del 27.11.2001, pag. 287.</p>
   <p n="85">(2) GU C 80 del 3.4.2002, pag. 21.</p>
   ... 
</div>
\end{verbatim}

\subsubsection{Corpus aggiornato e manutenuto}
JRC-Acquis è un corpus manutenuto e aggiornato manualmente, decine di migliaia di documenti riguardanti politiche europee su tematiche trasversali sono arricchiti da descrittori EuroVoc da specialisti del dominio. JRC-Acquis viene manutenuto perché utilizzato da diversi sistemi basati su tecnologie semantiche, nell'ambito della classificazione è utilizzato come corpo di addestramento del classificatore JRC-JEX, sistema ufficiale di classificazione dei documenti utilizzato dal parlamento europeo.

\subsubsection{Corpus parallelo multilingua}
JRC-Acquis è un corpus multilingua, questo vuol dire che è disponibile una versione per ciascuna lingua dell'Unione Europea. Il parallelismo è dato dal fatto che molti documenti sono disponibili in più lingue. Utilizzare corpus multilingua facilita il compito di sviluppare sistemi basati su tecnologie semantiche multilingua.

\subsubsection{Bridge dinamico tra EuroVoc e le categorie DCAT-AP}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.30]{img/eurovocdcat.png}
\caption{Bridge tra le categorie DCAT-AP ed EUROVOC}
\end{center}
\end{figure}

In appendice (\say{Associazione dei Temi DCAT-Microtesauri EuroVoc}) è possibile prendere nota delle corrispondenze tra i 13 temi definiti nell'ambito di DCAT-AP e i concetti (microtesauri) disponibili nel vocabolario EuroVoc (come indicato dal portale europeo https://www.europeandataportal.eu).
\footnotetext{Profilo italiano di DCAT-AP (DCAT Application Profile for data portals in Europe), Versione 1.0, Agenzia per l'Italia digitale, 2006}
\\
\\
JRC-Acquis è indicizzato con i concetti del vocabolario EuroVoc, pertanto è stato necessario sviluppare un modulo specializzato per la navigazione del tesauro al fine di ottenere i descrittori dei microtsauri associati ai descrittori dei concetti, per poi permutarli a loro volta nei rispettivi temi DCAT-AP.
\\
Il seguente modulo java:
\url{https://github.com/simonegasperoni/cata/blob/master/code/src/main/java/com/sciamlab/it/eurovoc/EVoc.java}
\\
è dedicato alla gestione di EuroVoc, fornisce funzionalità di base per interrogare il tesauro:
\begin{enumerate}
\item Dato un concetto si ottengono i microtesauri associati
\item Dato un microtesauro si ottiene il relativo tema DCAT-AP (EU Data Theme) 
\end{enumerate}

Un corpus etichettato con le voci di un tesauro come EuroVoc permette la realizzazione di associazioni tra concetti a vario livello di astrazione in modo dinamico, ad esempio qualora il microtesauro EuroVoc \say{2831 culture and religion} originariamente associato al tema Population \& Society (Popolazione e Società) dovesse passare in Education, culture and sport (Educazione, cultura e sport), basterebbe modificare il mapping tra i microtesauri e i temi all'interno del modulo specializzato.
\begin{lstlisting}
Map<String, EUNamedAuthorityDataTheme.Theme> EUROVOC_TO_DCAT_CATEGORIES = new HashMap<String, EUNamedAuthorityDataTheme.Theme>(){
		{
			put("0406",EUNamedAuthorityDataTheme.Theme.GOVE); 		
			...
			...
			...
			put("7626",EUNamedAuthorityDataTheme.Theme.INTR); 	
		}
};
\end{lstlisting}

Utilizzare Acquis garantisce anche un'altra forma di flessibilità, quella relativa al tesauro, infatti possiamo pensare al mapping tra i microtesauri e i temi in modo del tutto indipendente dalla versione di EuroVoc utilizzata. In realtà l'aggiornamento di EuroVoc non dovrebbe cambiare molto le cose dal punto di vista della classificazione se non si aggiorna anche il corpus (Acquis).

\newpage
\section{Elaborazione dei corpus}
Una fase preliminare in qualsiasi ambito di classificazione è l'elaborazione dei dati, la loro formattazione, per renderli fruibili da parte di sistema di classificazione.
\\Nel contesto di un sistema di text categorization vogliamo ottenere un sistema che utilizzi feature ben rappresentative del contenuto semantico dei documenti dai quali sono estratte.
\\
Le strade che si possono percorrere sono principalmente due:

\begin{description}
\item [Lemmatizzazione:] \say{quel complesso di operazioni che conducono a riunire tutte le forme sotto il rispettivo lemma}, intendendo per lemma \say{ciascuna parola-titolo o parola-chiave di un dizionario} e per forma ogni possibile diversa realizzazione grafica di un lemma.\footnotemark
\footnotetext{R. Busa, Fondamenti di informatica linguistica. Istituto Geografico De Agostini, 1987}
\\
Esistono delle convenzioni di lemmatizzazione proprie di ciascuna lingua: ad esempio in italiano è uso convenzionale che il lemma verbale sia la forma coniugata all'infinito presente attivo.
\\
La lemmatizzazione è, dunque, una pratica apparentemente facile, se non, addirittura, ovvia ed intuitiva: più, o meno tutti, infatti, esercitiamo quotidianamente la conoscenza della differenza tra lemma e forma. Tuttavia, alla prova dei fatti, la lemmatizzazione rivela una serie di problemi, il più delle volte non immaginabili prima di averne fatto esperienza diretta: queste difficoltà rendono il lemmatizzare un esercizio interessante, in quanto costringe chi lo esercita a riflettere su:

\begin{itemize}  
\item Quanti e quali automatismi l'uomo metta inconsciamente in atto ogni volta che parla, o scrive.
\item Quanto sia difficile formalizzare questi automatismi.
\end{itemize}

Esistono due tipi di lemmatizzazione:

\begin{itemize}  
\item \textbf{Lemmatizzazione morfologica:} analizza le forme di parole in isolamento, ovvero fuori dal contesto sintattico, fornendone tutti i valori che sono possibili in un dato sistema linguistico.
\item \textbf{Lemmatizzazione morfo-sintattica:} analizza le forme di parole entro il contesto sintattico. Non è mai ambigua, ma sempre univoca, in quanto l'immersione della forma nella frase ne precisa il valore. Quindi, mentre la lemmatizzazione morfologica è indipendente dal testo, la lemmatizzazione sintattica è, invece, legata al contesto.
\end{itemize}
  
\item [Stemmerizzazione:]Lo stemming è il processo usato per ridurre parole flesse al loro tema, il tema non deve necessariamente coincidere con la radice morfologica della parola: l'importante è che parole con una semantica strettamente correlata vengano mappate sullo stesso tema. Le tecniche di stemming sono studiate in informatica da quarant'anni, sono uno dei metodi di base per ridurre la dimensionalità dei documenti di testo.
\end{description}

Questo processo è reso particolarmente difficile e complesso a causa delle caratteristiche intrinseche di ambiguità del linguaggio umano. Per questo motivo il processo di elaborazione viene suddiviso in fasi diverse, tuttavia simili a quelle che si possono incontrare nel processo di elaborazione di un linguaggio di programmazione:


\begin{itemize}
\item \textbf{Analisi lessicale:} scomposizione di un'espressione linguistica in token (in questo caso le parole).
\item \textbf{Analisi grammaticale:} associazione delle parti del discorso a ciascuna parola nel testo.
\item \textbf{Analisi sintattica:} arrangiamento dei token in una struttura sintattica (ad albero: parse tree).
\item  \textbf{Analisi semantica:} assegnazione di un significato (semantica) alla struttura sintattica e, di conseguenza, all'espressione linguistica.
\end{itemize}

Nell'analisi semantica la procedura automatica che attribuisce all'espressione linguistica un significato tra i diversi possibili è detta disambiguazione.
La comprensione del linguaggio naturale è spesso considerata un problema IA-completo, poiché si pensa che il riconoscimento del linguaggio richieda una conoscenza estesa del mondo e una grande capacità di manipolarlo. Per questa ragione, la definizione di \say{comprensione} è uno dei maggiori problemi dell'elaborazione del linguaggio naturale.\footnotemark
\footnotetext{I. Chiari, Introduzione alla linguistica computazionale. Laterza, 2007}


\subsection{Disambiguazione}
Molte parole nel linguaggio naturale sono delle polisemie, vale a dire, possono avere più significati. Le tecniche finalizzate alla disambiguazione sono conosciute in letteratura come \textit{Word Sense Disambiguation} (WSD).
\\
La WSD può essere affrontata come un problema di classificazione, il senso corretto è la classe da predire la parola è rappresentata con un insieme (vettore) di feature in ingresso al classificatore, l'ingresso è di solito costituito da una rappresentazione della parola da disambiguare (target) e del contesto in cui si trova (un certo numero di parole a sinistra e destra della parola target). Il classificatore può essere stimato con tecniche di apprendimento automatico a partire da un dataset etichettato. Si possono usare diversi modelli per costruire il classificatore (Naïve Bayes, reti neurali, alberi di decisione...).
Il problema dell'approccio appena descritto è che il modello di apprendimento supervisionato che scegliamo di adottare potrebbe richiedere un training set troppo grande per essere sufficientemente preciso, per questo motivo esistono tecniche alternative come i metodi Dictionary-based.
\\
Tra i metodi Dictionary-based abbiamo l'algoritmo di Lesk (1986) che è un metodo molto semplice, basato sull'intuizione secondo cui un dizionario può fornire informazioni sul contesto legato ai sensi delle parole (le glosse).  

 
\subsection{Stemming}





\newpage
\section{Bayesian learning}
L'apprendimento bayesiano è un metodo computazionale di apprendimento automatico basato sul calcolo delle probabilità, che può fornire predizioni probabilistiche sfruttando i principi del teorema di Bayes per realizzare un apprendimento non supervisionato (mediante le Reti Bayesiane e i classificatori bayesiani).
\\
I classificatori bayesani sono metodi statistici di classificazione, predicono la probabilità che una data istanza appartenga ad una certa classe.
\\
I classificatori bayesiani sono metodi incrementali: ogni istanza dell’insieme di addestramento modifica in maniera incrementale la probabilità che una ipotesi sia corretta.
La conoscenza già acquisita può essere combinata facilmente con le nuove osservazioni basta aggiornare i conteggi. Questi metodi sono utilizzati ad esmpio in Mozilla o SpamAssassin per riconoscere le mail spam dalle mail ham.\footnotemark

\footnotetext{slide del corso di "Intelligenza artificiale", Università di Roma Tre, AA 2015-2016} 

Una probabilità è una misura su un insieme di eventi che soddisfa tre assiomi:

$$0≤P(E=e_i)≤1$$
$$\sum_{n=1}^{n} P(E=e_i)=1$$
$$P(E=e_1\cup E=e_2)=P(E=e_1)+P(E=e_2)\footnotemark$$
\footnotetext{gli eventi e1 ed e2 sono disgiunti}
Un modello probabilistico consiste in uno spazio di possibili esiti mutualmente esclusivi insieme alla misura di probabilità associata ad ogni esito.
Che tempo fa domani? esiti: \{SOLE, NUVOLE, PIOGGIA, NEVE\}, l'evento corrispondente ad una precipitazione è il sottoinsieme \{PIOGGIA, NEVE\}.
\begin{defn}
	La probabilità condizionale è definita come: $$P(B|A)=\frac{P(B \cap A)}{P(A)}$$
\end{defn}
\begin{defn}
	A e B sono condizionalmente indipendenti se $$P(B|A)=P(B)$$ o il suo equivalente $$P(A|B)=P(A)$$
\end{defn}

\subsection{Formula di Bayes}
Il teorema di Bayes (formula di Bayes o teorema della probabilità delle cause), proposto da Thomas Bayes, deriva da due teoremi fondamentali delle probabilità: il teorema della probabilità composta e il teorema della probabilità assoluta. Viene impiegato per calcolare la probabilità di una causa che ha scatenato l'evento verificato.
\begin{thm}
gli eventi A (per ogni i) sono stocasticamente indipendenti e sono spesso chiamati cause di E, vale a dire:
$$E \subset \bigcup\limits_{j=1}^{n} A_j $$
abbiamo
$$P(A_{i}|E)={\frac  {P(E|A_{i})P(A_{i})}{P(E)}}={\frac  {P(E|A_{i})P(A_{i})}{\sum {n \atop j=1}P(E|A_{j})P(A_{j})}}$$
Segue la dimostrazione.\footnotemark
Dalla teoria delle probabilità condizionali abbiamo:
$$(1)\,\,\, P(A_i|E)=\frac{P(A_i\cap E)}{P(E)}$$
$$(2)\,\,\, P(E|A_i)=\frac{P(A_i\cap E)}{P(A_i)} \Rightarrow P(A_i\cap E)=P(E|A_i)P(A_i)$$
andiamo a sostituire la (2) a numeratore della (1) ottenendo: 
$$P(A_i|E)=\frac{P(E|A_i)P(A_i)}{P(E)}$$
dato che
$$E \subset \bigcup\limits_{j=1}^{n} A_j $$
abbiamo che
$$P(E)=\sum {n \atop j=1}P(E|A_{j})P(A_{j})$$
perché
$$E=E \cap (\bigcup\limits_{j=1}^{n} A_j) \Rightarrow E=\bigcup\limits_{j=1}^{n} E \cap A_j$$  
infine abbiamo che
$$P(E)=P(\bigcup\limits_{j=1}^{n} E \cap A_j)$$ 
essendo gli eventi A incompatibili abbiamo 
$$P(E)=\sum {n \atop j=1}P(E \cap A_{j}) \Rightarrow 
\sum {n \atop j=1}P(E|A_{j})P(A_{j})$$

\footnotetext{Introduzione alla probabilità, Enzo Orsingher, Luisa Beghin, Carocci editore}

\end{thm}
\subsection{Classificatori bayesiani naïve}
L'assunzione di indipendenza rende i calcoli possibili consente di ottenere classificatori ottimali quando è soddisfatta ma è raramente soddisfatta in pratica.
Questa assunzione consentono di considerare le relazioni causali tra gli attributi, in realtà, si è visto che anche quando l’ipotesi di indipendenza non è soddisfatta, il classificatore naïve Bayes spesso fornisce ottimi risultati.

Sia X una istanza da classificare, e C1,...,Cn le possibili classi. I classificatori Bayesiani calcolano $$P(C_i|X)$$ come $$P(C_i|X)=\frac{P(X|C_i)P(C_i)}{P(X)}$$
Andiamo a cercare l'indice i per massimizzare la probabilità condizionata, ottenendo $$Max_i [P(C_i|X)]$$
P(X) è uguale per tutte le classi per cui non occorre calcolarla, P(Ci) si può calcolare facilmente sull’insieme dei dati di addestramento, si conta la percentuale di istanze di classe Ci sul totale.
Assunzione dei classificatori naïve: indipendenza degli
attributi.
Se X è composta dagli attributi 'a' con indice da 1 a m, otteniamo

$$P(X|C_i)=\prod_{j=1}^m P(A_j=a_j|C_i)$$
per il calcolo di 
$$P(A_j=a_j|C_i)$$
La formula che verrà utilizzata nella fase di predizione dal classificatore sarà pertanto:
$$v=Max_i [P(C_i)\prod_{j=1}^m P(A_j=a_j|C_i)]$$
dove $v$ è la categoria predetta.

Se gli A sono categorici, viene stimato come la frequenza relativa delle istanze che hanno quel determinato valore a con indice j tra tutte le istanze di C.
Se A è continuo, si assume che la probabilità segue una distribuzione Gaussiana, con media e varianza stimata a partire dalle istanze di classe C.

\begin{lstlisting}
#pseudo codice classificatore naif
#vettore dei target v[j]
def v[]
#vettore degli attributi a[i]
def a[]
#p(a[i]|v[j]) 
#probabilita' che occorra a[i] quando il documento  etichettato v[j]
#p(v[j]) probab. che occorra v[j]

def double p(e){
	"ritorna la stima probabilistica per l'evento e"
}

def void naif_bayes_learner(){
	for each j do {
		stima p(v[j])
		for each i do {
			stima p(a[i]|v[j])
		}
	}
}

#entry da classificare: x
def v nuova_classificazione(x){
		"ritorna v[j] tale che
		p(v[j])*p(a[1]|v[j])*..*p(a[i]|v[j])*..*p(a[n]|v[j])
		sia il massimo possibile"
}
\end{lstlisting}


\subsection{Stima delle probabilità (m-estimate)}
Quando calcoliamo le probabilità dobbiamo avere alcune accortezze, infatti, se un certo valore di un attributo non si verifica mai per una data classe quando arriva una nuova istanza X la probabilità sarà sempre nulla indipendentemente da quanto siano probabili i valori per gli altri attributi. Il problema delle frequenze nulle non è il solo nella stima delle probabilità in un classificatore bayesiano. Le probabilità tendono ad essere sottostimate in alcune circostanze, ad esempio:
$$P(wind=strong|playTennis=no)$$ stimato come $$\frac{n_C}{n}$$
Questa stima è buona in molti casi ma se abbiamo pochi esempi con playTennis=no la stima tenderà a zero.
Un modo per far fronte a tutte queste problematiche è mediante l'uso di una stima chiamata "m-estimate":
$$\frac{n_C+mp}{n+m}$$
p è la probabilità a priori, solitamente si assume p come il reciproco di k dove k è il numero di valori diversi per attributo
$$\frac{n_C+m\frac{1}{k}}{n+m}$$
m è la \textit{equivalent sample size}, come si può vedere dalla formula serve a determinare il peso di incidenza di p sui dati osservati. 

\subsection{Classificazione bayesiana di testi }
Il classificatore byaesiano sopradescritto trova applicazione nel campo della categorizzazione dei testi essendo - ad oggi - uno dei metodi più efficaci conosciuti.
Segue una trattazione sull'algoritmo che sfrutta le intuizioni probabilistiche bayesiane, altri esempi sono descritti da Lewis (1991), Lang (1995), Joachims (1996)\footnotemark
\footnotetext{Machine learning, Mc Graw Hill, 1997, Tom M. Mitchell}

Le istanze X che abbiamo considerato fin'ora possono ora essere considerati documenti testuali. Il training set da considerare è una collezione di documenti etichettati (classificati), su questa base di conoscenza si dovrà costruire un sistema di predizione per le entry dello spazio X.

Prendiamo in considerazione una collezione di testi, ad esempio 1000, dei quali 300 interessano ad una certa persone, mentre invece, gli altri 700 sono considerati non interessanti, questo può essere considerato un dataset per addestrare il nostro classificatore dove le categorie sono "like" e "dislike".

Due problemi fondamentali nella progettazione del classificatore bayesiano sono: la rappresentazione dei documenti, e la modalità di stima delle probabilità.

\subsection{Rappresentazione testi e stima delle probabilità}
Il modo più semplice di rappresentare i testi è mediante una raccolta - senza considerare l'ordine - di parole. Testi lunghi daranno luogo ad insiemi di attributi molto grandi, come vedremo questo non è un problema. Questo tipo di approccio è personalizzabile introducendo n-grammi di parole o la lemmatizzazione.
Ricollegandoci all'esempio di prima abbiamo: 
$$v=Max_i [P(C_i)\prod_{j=1}^m P(A_j=a_j|C_i)]$$
dove le categorie sono due: $$C_1:like, C_2:dislike$$
avremo dunque $$P(C_1)=\frac{300}{1000},\,\,\, P(C_2)=\frac{700}{1000}$$

Le probabilità condizionate sono semplicemente proporzionali alle frequenze con cui occorre un parola dentro tutti i documenti di una categoria.
La stima delle probabilità è una m-estimate nella quale consideriamo $$m=p=|Vocabolario|$$
$$\frac{w+1}{n+|Vocabolario|}$$
con n numero di parole di tutti i documenti di una determinata categoria, w numero di occorrenze di una data parola nell'insieme di parole di una data categoria.

In sintesi l'algoritmo di classificazione dei testi usa un classificatore bayesiano naïve con l'assunzione che la probabilità di occorrenza della parola è indipendente dalla posizione dentro i documenti.

Segue lo psudocodice di un approccio minimale:
\begin{lstlisting}
#pseudo codice classificatore naif per la categorizzazione testi
#vettore dei target v[j]
def v[]

#inizializzo vocabolario con tutti i vocaboli
def vocabolario=init_vocabolario()

def void naif_bayes_TEXT_learner(){
	#per ogni target v[j]
	for each j do {
		docs[j]="insieme dei documenti etichettati con v[j]"
		p(v[j])=|docs[j]|/|Esempi|
		text[j]="concateno tutti i docs[j]"
		n="numero di parole distinte dentro text[j]"
		
		#qui si calcolano i pesi per le parole
		for each parola in Vocabolario do{
			w="numero di volte che la parola occorre in text[j]"
			p(parola|v[j])=(w+1)/(n+|Vocabolario|)
		}
	}
}

#entry da classificare: x
nuova_classificazione(x)
\end{lstlisting}

\newpage
\section{Feature selection}
La selezione delle feature è il processo che ci porta a selezionare un sottoinsieme di feature (termini nella text classification), solo questo sottoinsieme sarà utilizzato come training set per i nostri classificatori.
I motivi principali per cui si procede ad una selezione delle feature sono due: innanzi tutto alcuni modelli possono essere addestrati solo con un insieme di feature  contenuto, data la loro complessità computazionale in fase di addestramento o predizione. In secondo luogo dobbiamo considerare che i classificatori tendono ad essere più precisi quando il numero delle feature è ridotto, molte feature, non solo non sono determinanti nell'individuazione della classe di appartenenza di un documento, ma possono addirittura introdurre un rumore (noise feature). Le noise feature sono quelle feature che occorrendo accidentalmente in una sola classe si rendono responsabili di errate generalizzazioni che colpiscono l'accuratezza della classificazione (Overfitting).
Introduction to Information Retrieval\footnotemark descrive tre procedure per la selezione di feature:
\begin{itemize}  
\item Mutual information
\item Chi square
\item Frequency based
\end{itemize}
Queste tre procedure ci permettono di ottenere una misura di utilità di ciascuna feature.

\footnotetext{C.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval, Cambridge University Press, 2008}

\subsection{Mutual information feature selection}
Mutual information di un termine t e una classe c è la misura di quanto la feature contribuisce a determinare la corretta classificazione

\subsection{$\chi^2$ feature selection}
Nella selezione delle feature $\chi^2$ si sfrutta l'intuizione statistica del test $\chi^2$ che serve a determinare il grado di indipendenza tra eventi, nel nostro caso tra feature e classe di appartenenza.
\[\chi^2(t,c)=\sum_{t \in [0,1]} \sum_{c \in [0,1]} \frac{(N_{c,t} - E_{c,t})^2}{E_{c,t}} \]  Il calcolo delle N riguarda le frequenze osservate: $N_{1,1}$ è il numero di documenti nei quali occorre il termine t nella classe c, $N_{1,0}$ è il numero di documenti nei quali non occorre il termine t nella classe c, $N_{0,1}$ è il numero di documenti nei quali occorre il termine t in tutte le classi eccetto la classe c, $N_{0,0}$ è il numero di documenti nei quali non occorre il termine t in tutte le classi eccetto c. 
\\
\\
$E_{c,t}$ sono le frequenze attese che il termine t e la classe c occorrano nello stesso documento essendo indipendenti:
\[ E_{c,t}= N \cdot P(t) \cdot P(c) \] 
ad esempio
\[ E_{1,1}= N \cdot \frac{N_{1,1} + N_{1,0}}{N} \cdot \frac{N_{1,1} + N_{0,1}}{N} \]

\subsection{Frequency-based feature selection}
Tra le tre procedure è la più semplice, consiste nell'andare a selezionare solo le feature che occorrono di più nelle varie classi. La frequenza può essere considerata come \say{Document frequency} (numero di documenti della classe che contengono una determinata feature), o come \say{Collection frequency} (numero di occorrenze della feature in una classe, senza considerare i documenti).
\say{Document frequency} è più appropriata per il modello di Bernoulli, mentre invece la \say{Collection frequency} è più appropriata per il modello multinomiale. Sebbene questo metodo sia molto meno complesso rispetto agli altri due, questo approccio introduce una limitazione, nel selezionare le feature potrebbe includere feature trasversali alle classi, queste feaure non danno un contributo informativo circa il legame tra la feature e la classe.

\newpage
\section{Evaluation}
La validazione di un sistema di text categorization è basata sulla classificazione di un campione di esempi che sono stati categorizzati manualmente da esperti del dominio. Quando si vuole valutare un sistema di text categorization si devono confrontare le categorie assegnate manualmente con le categorie assegnate dal sistema di text categorization.
\\
\\
Consideriamo la seguente casistica, nella quale diamo una rappresentazione binaria alle varie possibilità di assegnazione di una determinata categoria ad un determinato documento, distinguendo l'assegnazione del sistema da quella dell'esperto del dominio.
\\
\\
\begin{tabular}{l*{6}{c}r}
classe assegnata dall'esperto del dominio:  & SI & NO \\
\hline
classe assegnata dal classificatore: SI & TP & FP \\
classe assegnata dal classificatore: NO & FN & TN \\
\end{tabular}
\\
\\
Abbiamo TP: true positive, FP: false positive, FN: false negative e TN: true negative; questi valori vanno interpretati nel seguente modo:
\begin{itemize}
\item \textbf{true positive:} il sistema e l'esperto del dominio sono coerenti nell'assegnazione della categoria.
\item \textbf{false positive:} la categoria è assegnata dal sistema ma non dall'esperto del dominio.
\item  \textbf{true negative:} la categoria non è assegnata né dal sistema né dall'esperto del dominio.
\item \textbf{false negative:} la categoria è assegnata dall'esperto ma non dal sistema.
\end{itemize}


\subsection{Metriche per l'evaluation}
Piuttosto che avere tutti questi valori binari per ciascuna coppia documento-categoria siamo interessati ad avere quattro valori che rappresentino ciascuna delle quattro tipologie (true positive, false positive, true negative, false negative), pertanto è necessario sommare questi quattro valori per tutte le coppie documento-categoria. Questi quattro valori sono le grandezze che stanno alla base di varie metriche utili per l'evaluation del sistema di classificazione.

\[ Precision: \quad P= \frac{TP}{TP+FP} \]
\\
La Precision è una misura utile per quantificare la precisione del sistema sul campione di categorie selezionate a valle della classificazione, è il rapporto tra le categorie correttamente assegnate e tutte quelle assegnate.
\[ Recall: \quad R= \frac{TP}{TP+FN} \]
La Recall (in italiano \say{recupero}) è utile per misurare la frazione di categorie assegnate dall'esperto a valle della classificazione del sistema. La recall è il rapporto tra categorie assegnate e tutte le categorie di appartenenza di un documento. 
\\
Recall e Precision sono due metriche basilari nei sistemi di information retrieval, trovare un equilibrio tra queste due grandezze non è facile, è necessario un compromesso, in quanto, incrementando la precision abbiamo il declino della recall e viceversa. In una categorizzazione multi-label nella quale restituiamo poche categorie per documento è verosimile avere precision alta e recall bassa, mentre invece, se assegniamo molte categorie sarà più probabile avere recall alta a scapito di precision.


\[ Fallout: \quad F= \frac{FP}{FP+TN} \]

\[ Accuracy: \quad F= \frac{TP+TN}{TP+FP+FN+TN} \]
Utile per quantificare l'accuratezza generale del sistema.
\[ Error: \quad F= \frac{FP+FN}{TP+FP+FN+TN} \]
F-measure e Interpolated-break-even sono due grandezze che sintetizzano in un solo valore la precision e la recall di un sistema di classificazione:
\\
La F-measure (anche nota come F1-score) è la media armonica 
\[ F-measure: \quad F1= \frac{2PR}{P+R} \]
\[ Interpolated-break-even: \quad IB= \frac{P+R}{2} \]

Tra le due metriche è preferibile usare l'F1, in quanto più sensibile a valori di precision o recall molto bassi.



\subsection{K-fold-cross-validation}
Gli algoritmi di apprendimento supervisionato per la text categorization (Bayes-multinomiale, Bayes-Bernoulli, VSM-Rocchio, VSM-Knn, SVM, sono tutti supervisionati) hanno bisogno di un test dataset ed un training dataset per poter essere valutati. Questi due insiemi devono essere disgiunti per avere delle buone stime delle performance del sistema di classificazione.
\\
Training set: è un insieme di esempi usato per l'addestramento (apprendimento) del classificatore.
\\
Validation set: è un insieme di esempi usato per regolare i parametri del classificatore.
\\
Test set: è uni insieme di esempi usato per assegnare le performance di un classificatore addestrato.
\\
Se non separassimo gli insiemi di test e di validation la stima di errore di un modello sulla validazione sarà parziale (inferiore) in quanto il validation set è utilizzato per selezionare il modello finale, infine si può procedere alla valutazione del modello finale sul test set.
\footnotetext{Ricardo Gutierrez-Osuna, Introduction to Pattern Analysis, Texas A\&M University}
\\
La k-fold-cross-validation (in italiano validazione incrociata) è una tecnica utilizzabile in presenza di un training set abbastanza grande. In particolare la k-fold cross-validation consiste nella suddivisione del dataset totale in k parti di uguale numerosità e, ad ogni passo, la k-esima parte del dataset viene ad essere il test dataset, mentre la restante parte costituisce il training dataset. Così, per ognuna delle k parti (di solito k=10, k=5) si allena il modello, evitando quindi problemi di overfitting, ma anche di campionamento asimmetrico (e quindi affetto da bias) del training dataset, tipico della suddivisione del dataset in due sole parti (ovvero training e test dataset). In altre parole, si suddivide il campione osservato in gruppi di egual numerosità, si esclude iterativamente un gruppo alla volta e lo si cerca di predire con i gruppi non esclusi. Ciò al fine di verificare la bontà del modello di predizione utilizzato.

\newpage
\section{Aspetti software}
\subsection{Architettura}
\subsection{Deployment}

\newpage
\section{Bibliografia}


\newpage
\section{Appendice}
\subsection{Associazione Temi DCAT-Microtesauri EuroVoc}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): AGRI (Agriculture, Fisheries, Forestry \& Foods) 
\begin{verbatim}
	5606 agricultural policy
	5611 agricultural structures and production 
	5616 farming systems 
	5621 cultivation of agricultural land 
	5626 means of agricultural production 
	5631 agricultural activity
	5636 forestry 
	5641 fisheries
	6006 plant product
	6011 animal product 
	6021 beverages and sugar beverage sugar 
	6026 foodstuff 
	6031 agri-foodstuffs
	6036 food technology
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): ENER (Energy) 
\begin{verbatim}
	6606 energy policy 
	6616 oil industry 
	6621 electrical and nuclear industries 
	6626 soft energy
	6611 coal and mining industries
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): REGI (Regions \& Cities) 
\begin{verbatim}
	7206 Europe
	7211 regions of EU Member States 
	7216 America 
	7221 Africa
	7226 Asia and Oceania
	7231 economic geography 
	7236 political geography
	7241 overseas countries and territories
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): TRAN (Transport)
\begin{verbatim}
	4806 transport policy
	4811 organisation of transport 
	4816 land transport land transport 
	4821 maritime and inland waterway transport 
	4826 air and space transport
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): ECON (Economy \& Finance) 
\begin{verbatim}
	1606 economic policy 
	1611 economic growth 
	1616 regions and regional policy 
	1621 economic structure 
	1626 national accounts 
	1631 economic analysis 
	2406 monetary relations
	2411 monetary economics
	2416 financial institutions and credit
	2421 free movement of capital
	2426 financing and investment 
	2431 insurance
	2436 public finance and budget policy 
	2441 budget 
	2446 taxation
	2451 prices
	2006 trade policy
	2011 tariff policy 
	2016 trade supply
	2021 international trade 
	2026 consumption 
	2031 marketing
	4006 business organisation
	4011 business classification
	4016 legal form of organisations
	4021 management
	4026 accounting
	4031 competition 
	6806 industrial structures
	6811 chemistry 
	6816 iron
	6821 mechanical engineering
	6826 electronics and electrical engineering
	6831 building and public works
	6841 leather and textile industries 
	6846 miscellaneous industries 
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): INTR (International Issues) 
\begin{verbatim}
	7606 United Nations
	7611 European organisations 
	7616 extra-European organisations 
	7621 world organisations
	7626 non-governmental organisations
	0811 cooperation policy
	0816 international balance 
	0821 defence 
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): GOVE (Government \& Public Sector) 
\begin{verbatim}
	0406 political framework
	0411 political party 
	0416 electoral procedure and voting election
	0421 parliament 
	0426 parliamentary proceedings
	0431 politics and public safety 
	0436 executive power and public service 
	1006 Community institutions and European civil service 
	1011 European Union law 
	1016 European construction
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): JUST (Justice, Legal System \& Public Safety) 
\begin{verbatim}
	1206 sources and branches of the law 
	1211 civil law 
	1216 criminal law 
	1221 justice access to the courts 
	1226 organisation of the legal system 
	1236 rights and freedoms
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): ENVI (Environment) 
\begin{verbatim}
	5206 environmental policy
	5211 natural environment
	5216 deterioration of the environment 
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): EDUC (Education, Culture \& Sport) 
\begin{verbatim}
	3206 education
	3211 teaching 
	3216 organisation of teaching 
	3221 documentation 
	3226 communications
	3231 information and information processing
	3236 information technology and data processing 
	2831 culture and religion arts cultural policy culture
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): HEAL (Health) 
\begin{verbatim}
	2841 health
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): SOCI (Population \& Society) 
\begin{verbatim}
	2806 family 
	2811 migration internal 
	2816 demography and population composition 
	2821 social framework
	2826 social affairs
	2836 social protection
	2846 construction and town planning 
	4406 employment
	4411 labour market 
	4416 organisation of work and working conditions 
	4421 personnel management and staff remuneration
	4426 labour law and labour relations
\end{verbatim}
Microtesauri EuroVoc associati al tema DCAT-AP (EU Data Themes): TECH (Science \& Technology)
\begin{verbatim}
	3606 natural and applied sciences 
	3611 humanities behavioural 
	6406 production 
	6411 technology and technical regulations
\end{verbatim}

\end{document}
